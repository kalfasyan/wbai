{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kalfasyan/anaconda3/envs/wbai/lib/python3.6/site-packages/torchaudio/backend/utils.py:54: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "  '\"sox\" backend is being deprecated. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total RAM      : 31.21 GB\n",
      "Available RAM  : 22.91 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import datasets, transforms\n",
    "from tsai.all import *\n",
    "\n",
    "torchaudio.USE_SOUNDFILE_LEGACY_INTERFACE = False\n",
    "torchaudio.set_audio_backend('soundfile')\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import psutil\n",
    "import requests\n",
    "\n",
    "from utils import *\n",
    "from wingbeat_datasets import *\n",
    "from wingbeat_models import *\n",
    "\n",
    "print(f'Total RAM      : {bytes2GB(psutil.virtual_memory().total):5.2f} GB')\n",
    "print(f'Available RAM  : {bytes2GB(psutil.virtual_memory().available):5.2f} GB\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 35\n",
    "batch_size = 32\n",
    "batch_size_val = batch_size * 2\n",
    "validation_split = .2\n",
    "shuffle_dataset = True\n",
    "num_workers = psutil.cpu_count()\n",
    "random_seed= 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 29002 in dataset: Melanogaster_RL/Y, and 1 label(s): ['Y']\n",
      "Label(s) changed to 0\n",
      "Found 24763 in dataset: Melanogaster_RL/Z, and 1 label(s): ['Z']\n",
      "Label(s) changed to 0\n",
      "Found 21940 in dataset: Suzukii_RL/L, and 1 label(s): ['L']\n",
      "Label(s) changed to 1\n",
      "Found 14348 in dataset: Suzukii_RL/R, and 1 label(s): ['R']\n",
      "Label(s) changed to 1\n"
     ]
    }
   ],
   "source": [
    "dmel1 = WingbeatsDataset(\"Melanogaster_RL/Y\", custom_label=0,transform=FilterWingbeat('bandpass'))\n",
    "dmel2 = WingbeatsDataset(\"Melanogaster_RL/Z\", custom_label=0,transform=FilterWingbeat('bandpass'))\n",
    "\n",
    "dsuz1 = WingbeatsDataset(\"Suzukii_RL/L\", custom_label=1,transform=FilterWingbeat('bandpass'))\n",
    "dsuz2 = WingbeatsDataset(\"Suzukii_RL/R\", custom_label=1,transform=FilterWingbeat('bandpass'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_dataset = ConcatDataset([dmel1, dsuz1])\n",
    "\n",
    "train_size = int(0.8 * len(transformed_dataset))\n",
    "valid_size = len(transformed_dataset) - train_size\n",
    "train_dataset, valid_dataset = torch.utils.data.random_split(transformed_dataset, [train_size, valid_size])\n",
    "test_dataset = ConcatDataset([dmel2, dsuz2])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size_val, num_workers=num_workers)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size_val, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1dNet(\n",
      "  (conv1): Conv1d(1, 16, kernel_size=(3,), stride=(1,))\n",
      "  (bn1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv1d(16, 32, kernel_size=(3,), stride=(1,))\n",
      "  (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv1d(32, 64, kernel_size=(3,), stride=(1,))\n",
      "  (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv4): Conv1d(64, 128, kernel_size=(3,), stride=(1,))\n",
      "  (bn4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool4): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv5): Conv1d(128, 256, kernel_size=(3,), stride=(1,))\n",
      "  (bn5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool5): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (avgPool): AvgPool1d(kernel_size=(154,), stride=(154,), padding=(0,))\n",
      "  (fc1): Linear(in_features=256, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Conv1dNet()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, betas=(0.9, 0.999))\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, verbose=True)\n",
    "early_stopping = EarlyStopping(patience=7, verbose=False)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import preprocessing\n",
    "# import itertools\n",
    "\n",
    "# le = preprocessing.LabelEncoder()\n",
    "# all_labels = [transformed_dataset.datasets[i].labels for i in range(len(transformed_dataset.datasets))]\n",
    "# all_labels = list(itertools.chain.from_iterable(all_labels))\n",
    "# le.fit(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on gpu: True\n"
     ]
    }
   ],
   "source": [
    "# Choosing whether to train on a gpu\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "print(f'Train on gpu: {train_on_gpu}')# Number of gpus\n",
    "model = model.to('cuda', dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train_acc: 83.76% loss: 0.399,  val_loss: 21.126 val_acc: 57.17%\n",
      "Epoch 1: train_acc: 90.45% loss: 0.239,  val_loss: 41.426 val_acc: 57.17%\n",
      "EarlyStopping counter: 1 out of 7\n",
      "Epoch 2: train_acc: 92.33% loss: 0.119,  val_loss: 22.338 val_acc: 57.17%\n",
      "EarlyStopping counter: 2 out of 7\n",
      "Epoch 3: train_acc: 93.48% loss: 0.070,  val_loss: 43.447 val_acc: 57.17%\n",
      "EarlyStopping counter: 3 out of 7\n",
      "Epoch     5: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 4: train_acc: 94.17% loss: 0.068,  val_loss: 25.790 val_acc: 57.40%\n",
      "EarlyStopping counter: 4 out of 7\n",
      "Epoch 5: train_acc: 96.00% loss: 0.023,  val_loss: 0.001 val_acc: 71.99%\n",
      "Epoch 6: train_acc: 96.31% loss: 0.015,  val_loss: 0.767 val_acc: 67.41%\n",
      "EarlyStopping counter: 1 out of 7\n",
      "Epoch 7: train_acc: 96.56% loss: 0.015,  val_loss: 0.000 val_acc: 80.48%\n",
      "Epoch 8: train_acc: 96.74% loss: 0.010,  val_loss: 0.001 val_acc: 91.30%\n",
      "EarlyStopping counter: 1 out of 7\n",
      "Epoch 9: train_acc: 96.90% loss: 0.012,  val_loss: 0.671 val_acc: 67.27%\n",
      "EarlyStopping counter: 2 out of 7\n",
      "Epoch 10: train_acc: 97.12% loss: 0.018,  val_loss: 0.001 val_acc: 84.55%\n",
      "EarlyStopping counter: 3 out of 7\n",
      "Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 11: train_acc: 97.30% loss: 0.022,  val_loss: 0.123 val_acc: 91.32%\n",
      "EarlyStopping counter: 4 out of 7\n",
      "Epoch 12: train_acc: 97.59% loss: 0.007,  val_loss: 0.004 val_acc: 95.70%\n",
      "EarlyStopping counter: 5 out of 7\n",
      "Epoch 13: train_acc: 97.70% loss: 0.014,  val_loss: 0.003 val_acc: 95.66%\n",
      "EarlyStopping counter: 6 out of 7\n",
      "Epoch 14: train_acc: 97.71% loss: 0.005,  val_loss: 0.007 val_acc: 95.12%\n",
      "EarlyStopping counter: 7 out of 7\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "# Model training\n",
    "for epoch in range(num_epochs):\n",
    "    # Going through the training set\n",
    "    correct_train = 0\n",
    "    model.train()\n",
    "    for x_batch,y_batch,path_batch in train_dataloader:        \n",
    "\n",
    "        y_batch = torch.as_tensor(y_batch).type(torch.LongTensor)\n",
    "        x_batch,y_batch = x_batch.cuda(), y_batch.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        pred = model(x_batch)\n",
    "        loss = criterion(pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        correct_train += (pred.argmax(axis=1) == y_batch).float().sum().item()\n",
    "    train_accuracy = correct_train / (len(train_dataloader)*batch_size) * 100.\n",
    "    # Going through the validation set\n",
    "    correct_valid = 0\n",
    "    model.eval()\n",
    "    for x_batch,y_batch,path_batch in valid_dataloader:\n",
    "        \n",
    "        y_batch = torch.as_tensor(y_batch).type(torch.LongTensor)\n",
    "        x_batch,y_batch = x_batch.cuda(), y_batch.cuda()\n",
    "\n",
    "        pred = model(x_batch)\n",
    "        val_loss = criterion(pred, y_batch)\n",
    "        correct_valid += (pred.argmax(axis=1) == y_batch).float().sum().item()\n",
    "    valid_accuracy = correct_valid / (len(valid_dataloader)*batch_size_val) * 100.\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # Printing results\n",
    "    print(f\"Epoch {epoch}: train_acc: {train_accuracy:.2f}% loss: {loss:.3f},  val_loss: {val_loss:.3f} val_acc: {valid_accuracy:.2f}%\")\n",
    "\n",
    "    early_stopping(val_loss, model)\n",
    "\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89.49652777777779\n"
     ]
    }
   ],
   "source": [
    "correct_test = 0\n",
    "model.eval()\n",
    "for x_batch,y_batch,path_batch in test_dataloader:\n",
    "\n",
    "    y_batch = torch.as_tensor(y_batch).type(torch.LongTensor)\n",
    "    x_batch,y_batch = x_batch.cuda(), y_batch.cuda()\n",
    "\n",
    "    pred = model(x_batch)\n",
    "    val_loss = criterion(pred, y_batch)\n",
    "    correct_test += (pred.argmax(axis=1) == y_batch).float().sum().item()\n",
    "test_accuracy = correct_test / (len(test_dataloader)*batch_size_val) * 100.\n",
    "print(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
